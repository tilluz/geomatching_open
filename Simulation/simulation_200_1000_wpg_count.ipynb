{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geovoronoi import voronoi_regions_from_coords\n",
    "import palettable\n",
    "import scipy.spatial as spatial\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# load propagation models\n",
    "%run ./propagation_model_extended_hata.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myround(x):\n",
    "    return (math.ceil(10*x) - 0.5)/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "pixel_x = 100\n",
    "pixel_y = 100\n",
    "pop_rural = int(50*10**4 * (pixel_x/100)**2)\n",
    "pop_urban = int(50*10**4 * (pixel_x/100)**2)\n",
    "ms_per_pixel_rural = 1 * pop_rural\n",
    "ms_per_pixel_urban = 1 * pop_urban\n",
    "bts_count_rural = ms_per_pixel_rural / 10000\n",
    "bts_count_urban = ms_per_pixel_urban / 5000\n",
    "urban_tile_size = 5\n",
    "rural_tile_size = 20\n",
    "p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pop_rural + pop_urban\n",
    "\n",
    "bts_count = bts_count_rural + bts_count_urban\n",
    "\n",
    "# Administrative frame\n",
    "### Area classification: 0 = urban, 1 = rural\n",
    "\n",
    "base_map = gpd.GeoDataFrame(columns = ['area_id','area_type'], crs={'init': 'epsg:4326'}, geometry=[])\n",
    "\n",
    "target_urban = int(pixel_x*p/urban_tile_size)\n",
    "target_rural = int(pixel_x*(1 - p)/rural_tile_size)\n",
    "i = 0\n",
    "\n",
    "for x in range(target_urban):\n",
    "\n",
    "    for y in range(target_urban):\n",
    "        \n",
    "        lat = [y*p*pixel_y/target_urban, (y+1)*p*pixel_y/target_urban, (y+1)*p*pixel_y/target_urban, y*p*pixel_y/target_urban, y*p*pixel_y/target_urban]\n",
    "        lon = [x*p*pixel_x/target_urban, x*p*pixel_x/target_urban, (x+1)*p*pixel_x/target_urban, (x+1)*p*pixel_x/target_urban, x*p*pixel_x/target_urban]\n",
    "\n",
    "        base_map = base_map.append(gpd.GeoDataFrame(data = {'area_id' : [i], 'area_type' : 0}, index = [i], crs={'init': 'epsg:4326'}, geometry=[Polygon(zip(lon, lat))]))\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "for x in range(int(pixel_x*p/rural_tile_size)):\n",
    "\n",
    "    for y in range(target_rural):\n",
    "        \n",
    "        lat = [(p + (1 - p)/target_rural*y)*pixel_y, (p + (1 - p)/target_rural*(y+1))*pixel_y, (p + (1 - p)/target_rural*(y+1))*pixel_y, (p + (1 - p)/target_rural*y)*pixel_y, (p + (1 - p)/target_rural*y)*pixel_y]\n",
    "        lon = [x*(1-p)*pixel_x/target_rural, x*(1-p)*pixel_x/target_rural, (x+1)*(1-p)*pixel_x/target_rural, (x+1)*(1-p)*pixel_x/target_rural, x*(1-p)*pixel_x/target_rural]\n",
    "\n",
    "        base_map = base_map.append(gpd.GeoDataFrame(data = {'area_id' : [i], 'area_type' : 1}, index = [i], crs={'init': 'epsg:4326'}, geometry=[Polygon(zip(lon, lat))]))\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "for x in range(target_rural):\n",
    "\n",
    "    for y in range(int(pixel_x*p/rural_tile_size)):\n",
    "        \n",
    "        lat = [y*(1-p)*pixel_y/target_rural, (y+1)*(1-p)*pixel_y/target_rural, (y+1)*(1-p)*pixel_y/target_rural, y*(1-p)*pixel_y/target_rural, y*(1-p)*pixel_y/target_rural]\n",
    "        lon = [(p + (1 - p)/target_rural*x)*pixel_x, (p + (1 - p)/target_rural*x)*pixel_x, (p + (1 - p)/target_rural*(x+1))*pixel_x, (p + (1 - p)/target_rural*(x+1))*pixel_x, (p + (1 - p)/target_rural*x)*pixel_x]\n",
    "        \n",
    "        base_map = base_map.append(gpd.GeoDataFrame(data = {'area_id' : [i], 'area_type' : 1}, index = [i], crs={'init': 'epsg:4326'}, geometry=[Polygon(zip(lon, lat))]))\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "for x in range(target_rural):\n",
    "\n",
    "    for y in range(target_rural):\n",
    "        \n",
    "        lat = [(p + (1 - p)/target_rural*y)*pixel_y, (p + (1 - p)/target_rural*(y+1))*pixel_y, (p + (1 - p)/target_rural*(y+1))*pixel_y, (p + (1 - p)/target_rural*y)*pixel_y, (p + (1 - p)/target_rural*y)*pixel_y]\n",
    "        lon = [(p + (1 - p)/target_rural*x)*pixel_x, (p + (1 - p)/target_rural*x)*pixel_x, (p + (1 - p)/target_rural*(x+1))*pixel_x, (p + (1 - p)/target_rural*(x+1))*pixel_x, (p + (1 - p)/target_rural*x)*pixel_x]\n",
    "        \n",
    "        base_map = base_map.append(gpd.GeoDataFrame(data = {'area_id' : [i], 'area_type' : 1}, index = [i], crs={'init': 'epsg:4326'}, geometry=[Polygon(zip(lon, lat))]))\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "base_map['area_km2'] = base_map.area\n",
    "\n",
    "park_x_low = int(0.5*pixel_x)\n",
    "park_y_low = int(0.5*pixel_y)\n",
    "park_x_high = int(0.75*pixel_x)\n",
    "park_y_high = int(0.75*pixel_y)\n",
    "\n",
    "# Grid frame\n",
    "\n",
    "xmin = 0\n",
    "ymin = 0\n",
    "xmax = pixel_x*10\n",
    "ymax = pixel_y*10\n",
    "\n",
    "length = 1\n",
    "width = 1\n",
    "\n",
    "cols = list(range(int(np.floor(xmin)), int(np.ceil(xmax)), width))\n",
    "rows = list(range(int(np.floor(ymin)), int(np.ceil(ymax)), length))\n",
    "rows.reverse()\n",
    "\n",
    "polygons = []\n",
    "\n",
    "for x in cols:    \n",
    "    for y in rows:\n",
    "        polygons.append(Polygon([(x/10,y/10), ((x+width)/10, y/10), ((x+width)/10, (y+length)/10), (x/10, (y+length)/10)]))\n",
    "        \n",
    "grid_map = gpd.GeoDataFrame(index=[], crs={'init': 'epsg:4326'}, geometry = polygons)\n",
    "\n",
    "polygons = None\n",
    "\n",
    "grid_map.insert(loc=0, column='grid_id', value=list(range(len(grid_map))))\n",
    "\n",
    "grid_centroid = grid_map.copy()\n",
    "\n",
    "grid_centroid['geometry'] = grid_centroid.centroid\n",
    "\n",
    "df = gpd.tools.sjoin(\n",
    "    base_map[['area_id', 'area_type', 'geometry']], grid_centroid[['grid_id', 'geometry']], how=\"left\", op='intersects').reset_index().drop(\n",
    "    columns = ['index', 'index_right', 'geometry']).astype({'grid_id' : 'uint32', 'area_id' : 'uint16', 'area_type' : 'uint8'}).drop_duplicates(subset = 'grid_id')\n",
    "\n",
    "grid_centroid['x'] = np.around(grid_centroid.geometry.x, decimals = 2)\n",
    "grid_centroid['y'] = np.around(grid_centroid.geometry.y, decimals = 2)\n",
    "\n",
    "# Poverty frame\n",
    "\n",
    "xmin = 0\n",
    "ymin = 0\n",
    "xmax = pixel_x\n",
    "ymax = pixel_y\n",
    "\n",
    "length = 4\n",
    "width = 4\n",
    "\n",
    "cols = list(range(int(np.floor(xmin)), int(np.ceil(xmax)), width))\n",
    "rows = list(range(int(np.floor(ymin)), int(np.ceil(ymax)), length))\n",
    "rows.reverse()\n",
    "\n",
    "polygons = []\n",
    "\n",
    "for x in cols:    \n",
    "    for y in rows:\n",
    "        polygons.append(Polygon([(x, y), ((x+width), y), ((x+width), (y+length)), (x, (y+length))]))\n",
    "        \n",
    "poverty_map = gpd.GeoDataFrame(index=[], crs={'init': 'epsg:4326'}, geometry = polygons)\n",
    "\n",
    "polygons = None\n",
    "\n",
    "poverty_map.insert(loc=0, column='pov_id', value=list(range(len(poverty_map))))\n",
    "\n",
    "poverty_df = gpd.tools.sjoin(\n",
    "    poverty_map[['pov_id', 'geometry']], grid_centroid[['grid_id', 'geometry']], how=\"left\", op='intersects').reset_index().drop(\n",
    "    columns = ['index', 'index_right', 'geometry']).astype({'grid_id' : 'uint32', 'pov_id' : 'uint16'})\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_map.plot(edgecolor = 'white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_small(df_full, pixel_x, pixel_y, park_x_low, park_y_low, park_x_high, park_y_high, \n",
    "                     pop_urban, pop_rural, r):\n",
    "    \n",
    "    # Urban Population\n",
    "    mean = [int(pixel_x/10), int(pixel_x/10)]\n",
    "    cov = [[pixel_x/2, 0], [0, pixel_y/2]]\n",
    "\n",
    "    x_gaussian, y_gaussian = np.random.RandomState().multivariate_normal(mean, cov, 10000000).T\n",
    "\n",
    "    gaussian = pd.DataFrame({'x': x_gaussian, 'y': y_gaussian})\n",
    "    gaussian = gaussian[(gaussian['x'] >= 0) & (gaussian['y'] >= 0) & (gaussian['x'] < pixel_x) & (gaussian['y'] < pixel_y)]\n",
    "    gaussian = gaussian[0:pop_urban]\n",
    "\n",
    "    # Rural Population\n",
    "    towns = pd.DataFrame()\n",
    "\n",
    "    while towns.shape[0] < pop_rural:\n",
    "\n",
    "        mean = [np.random.RandomState().uniform(0, pixel_x, 1)[0], np.random.RandomState().uniform(0, pixel_y, 1)[0]]\n",
    "\n",
    "        # No towns in national park\n",
    "        if (mean[0] >= park_x_low) & (mean[1] >= park_y_low) & (mean[0] < park_x_high) & (mean[1] < park_y_high):\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            cov_size = np.random.RandomState().normal(10000, 10000, 1).astype('uint16')[0]\n",
    "\n",
    "            cov = [[abs(np.random.RandomState().normal(cov_size/5000, cov_size/50000, 1))[0], 0],\n",
    "                   [0, abs(np.random.RandomState().normal(cov_size/5000, cov_size/50000, 1))[0]]]\n",
    "\n",
    "            x_gaussian, y_gaussian = np.random.RandomState().multivariate_normal(mean, cov, 1000000).T\n",
    "\n",
    "            town = pd.DataFrame({'x': x_gaussian, 'y': y_gaussian})\n",
    "            town = town[(town['x'] >= 0) & (town['y'] >= 0) & (town['x'] < pixel_x) & (town['y'] < pixel_y)]\n",
    "            town = town[0:cov_size]\n",
    "            towns = towns.append(town)\n",
    "\n",
    "    towns = towns[0:int(0.995*pop_rural)]\n",
    "\n",
    "    x_uni = np.random.RandomState().uniform(0, pixel_x, 100000)\n",
    "    y_uni = np.random.RandomState().uniform(0, pixel_y, 100000)\n",
    "\n",
    "    uni = pd.DataFrame({'x': x_uni, 'y': y_uni})\n",
    "\n",
    "    uni = uni[~((uni.x >= park_x_low) & (uni.y >= park_y_low) & (uni.x < park_x_high) & (uni.y < park_y_high))]\n",
    "\n",
    "    towns = towns.append(uni[0:(pop_rural - towns.shape[0])])\n",
    "\n",
    "    gaussian = gaussian.append(towns).reset_index().drop(columns = ['index'])\n",
    "\n",
    "    x_gaussian = None\n",
    "    y_gaussian = None\n",
    "    x_uni = None\n",
    "    y_uni = None\n",
    "    towns = None\n",
    "    uni = None\n",
    "\n",
    "    gaussian.insert(loc=0, column='id', value=list(range(len(gaussian))))\n",
    "\n",
    "    # BTS Locations\n",
    "    sample_size = 0.01\n",
    "\n",
    "    bts_sample = gaussian[gaussian['id'].isin(np.random.choice(gaussian['id'], int(sample_size*gaussian.shape[0]), replace=False))].copy()\n",
    "\n",
    "    kmeans = KMeans(n_clusters=int(bts_count), random_state = np.random.RandomState()).fit(bts_sample[['x', 'y']])\n",
    "    bts_sample['bts'] = kmeans.labels_\n",
    "\n",
    "    kmeans_centroid = pd.DataFrame(data = {'x': [row[0] for row in kmeans.cluster_centers_], 'y': [row[1] for row in kmeans.cluster_centers_]})\n",
    "    kmeans_centroid['bts'] = list(range(0, kmeans.labels_.max() + 1))\n",
    "\n",
    "    # Population per Grid\n",
    "    gaussian['x'] = np.around(gaussian['x'].apply(lambda x: myround(x)), decimals = 2)\n",
    "    gaussian['y'] = np.around(gaussian['y'].apply(lambda x: myround(x)), decimals = 2)\n",
    "\n",
    "    grid_count = gaussian.merge(\n",
    "        grid_centroid[['grid_id', 'x', 'y']], on = ['x', 'y'], how = 'left').groupby(['grid_id', 'x', 'y'])['id'].count().reset_index().rename(columns={'id':'pop'})\n",
    "\n",
    "    df = df_full.merge(grid_count, on = 'grid_id', how = 'left').fillna(0)\n",
    "\n",
    "    # BTS Specifications\n",
    "    nA = np.array(list(zip(grid_centroid.x, grid_centroid.y)) )\n",
    "    nB = np.array(list(zip(kmeans_centroid.x, kmeans_centroid.y)) )\n",
    "    btree = spatial.cKDTree(nB)\n",
    "\n",
    "    links_all = pd.DataFrame()\n",
    "\n",
    "    for i in range(1,6):\n",
    "\n",
    "        dist, idx = btree.query(nA, k = [i])\n",
    "\n",
    "        temp = grid_centroid[['grid_id']].copy()\n",
    "\n",
    "        temp['bts'] = kmeans_centroid.loc[idx.flatten(), kmeans_centroid.columns == 'bts'].reset_index().drop(columns = ['index'])\n",
    "        temp['distance'] = dist.flatten()\n",
    "        temp['k'] = i\n",
    "\n",
    "        links_all = links_all.append(temp)\n",
    "\n",
    "    nA = None\n",
    "    nB = None\n",
    "    btree = None\n",
    "    temp = None\n",
    "\n",
    "    links_all = links_all.merge(df, on = 'grid_id', how = 'left').astype({'grid_id' : 'uint32', 'bts' : 'uint16', 'k' : 'uint8', 'pop' : 'uint8'})\n",
    "\n",
    "    bts_specs = links_all[links_all.k == 1].groupby('bts')['grid_id'].count().reset_index().rename(columns = {'grid_id' : 'grid_count'})\n",
    "\n",
    "    bts_specs = kmeans_centroid.merge(bts_specs, on = 'bts', how = 'left')\n",
    "\n",
    "    bts_specs['urbanity'] = np.where(bts_specs['grid_count'] <= np.quantile(bts_specs['grid_count'], 0.5), 0, 2)\n",
    "    bts_specs.loc[bts_specs.urbanity != 0, 'urbanity'] = np.where(bts_specs.loc[bts_specs.urbanity != 0, 'grid_count'] > np.quantile(bts_specs['grid_count'], 0.95), 1, 2)\n",
    "\n",
    "    bts_specs.loc[bts_specs.urbanity == 0, 'f'] = np.random.RandomState().binomial(1, 0.5, bts_specs.loc[bts_specs.urbanity == 0].shape[0]) #0.9\n",
    "    bts_specs.loc[bts_specs.urbanity == 2, 'f'] = np.random.RandomState().binomial(1, 0.3, bts_specs.loc[bts_specs.urbanity == 2].shape[0]) #0.7\n",
    "    bts_specs.loc[bts_specs.urbanity == 1, 'f'] = np.random.RandomState().binomial(1, 0, bts_specs.loc[bts_specs.urbanity == 1].shape[0])\n",
    "\n",
    "    bts_specs.loc[bts_specs.f == 0, 'f'] = 900\n",
    "    bts_specs.loc[bts_specs.f == 1, 'f'] = 2100\n",
    "\n",
    "    bts_specs.loc[bts_specs.f == 900, 'h_tx'] = np.random.RandomState().uniform(20, 50, bts_specs[bts_specs.f == 900].shape[0]).astype('int')\n",
    "    bts_specs.loc[bts_specs.f == 2100, 'h_tx'] = np.random.RandomState().uniform(15, 60, bts_specs[bts_specs.f == 2100].shape[0]).astype('int')\n",
    "\n",
    "    bts_specs.loc[bts_specs.f == 900, 'p_tx'] = np.random.RandomState().uniform(43, 47, bts_specs[bts_specs.f == 900].shape[0]).astype('int')\n",
    "    bts_specs.loc[bts_specs.f == 2100, 'p_tx'] = np.random.RandomState().uniform(40, 47, bts_specs[bts_specs.f == 2100].shape[0]).astype('int')\n",
    "\n",
    "    # Poverty Rate\n",
    "    \n",
    "    df = df[df['pop'] != 0]\n",
    "\n",
    "    df = df.merge(poverty_df, on = 'grid_id', how = 'left')\n",
    "\n",
    "    poverty_probability = np.random.RandomState().uniform(0, 1, 100000)\n",
    "\n",
    "    pov_factor = df.groupby(['pov_id'])['pop'].sum().reset_index()\n",
    "    pov_factor['pop_factor'] = 1 - (pov_factor['pop'] / max(pov_factor['pop']))\n",
    "\n",
    "    for i in df['pov_id'].astype('int').unique():\n",
    "\n",
    "        df.loc[(df.pov_id == i) & (df.pop != 0), 'poverty_rate'] = np.random.RandomState().normal(\n",
    "            pov_factor.loc[pov_factor['pov_id'] == i, 'pop_factor']*poverty_probability[i], 0.5, len(df.loc[(df.pov_id == i) & (df.pop != 0)]))\n",
    "    \n",
    "    df.loc[df.poverty_rate < 0, 'poverty_rate'] = 0\n",
    "    df.loc[df.poverty_rate > 1, 'poverty_rate'] = 1\n",
    "\n",
    "    df['poverty_model'] = df['poverty_rate']\n",
    "    \n",
    "    df = df[df['pop'] != 0]\n",
    "\n",
    "    links = links_all[links_all['grid_id'].isin(df['grid_id'])].copy()\n",
    "\n",
    "    links = links.merge(bts_specs[['bts', 'urbanity', 'f', 'h_tx', 'p_tx']], on = 'bts', how = 'left')\n",
    "    \n",
    "    links_all = None\n",
    "\n",
    "    # Simulation: True Coverage\n",
    "    for i in bts_specs.index:\n",
    "\n",
    "        for j in np.arange(0.1, 10000, 0.1):\n",
    "\n",
    "            # Bagged path loss estimation\n",
    "            alpha = alpha_value(bts_specs.loc[i, 'f'], j, bts_specs.loc[i, 'h_tx'])\n",
    "            path_loss_hata = calculate_path_loss_hata_novar(bts_specs.loc[i, 'f'], bts_specs.loc[i, 'urbanity'], j, bts_specs.loc[i, 'h_tx'], 1, alpha)\n",
    "\n",
    "            if (bts_specs.loc[i, 'p_tx'] - path_loss_hata) < -110:\n",
    "                bts_specs.loc[i, 'max_range'] = j\n",
    "                break\n",
    "\n",
    "        true_coverage = bts_specs[['bts', 'urbanity', 'max_range']].rename(columns = {'max_range' : 'distance'})\n",
    "\n",
    "    true_coverage = true_coverage[true_coverage['distance'] != 0]\n",
    "    true_coverage = true_coverage.merge(bts_specs[['bts', 'x', 'y']]).rename(columns = {'bts' : 'true_bts'})\n",
    "    true_coverage = gpd.GeoDataFrame(true_coverage, crs={'init': 'epsg:4326'}, geometry=[Point(xy) for xy in zip(true_coverage.x, true_coverage.y)])\n",
    "    true_coverage['geometry'] = true_coverage.apply(lambda x : x.geometry.buffer(x.distance), axis = 1)\n",
    "    true_coverage['true_bts_km2'] = true_coverage.area\n",
    "\n",
    "    df_eval_cov = true_coverage[['true_bts', 'urbanity', 'true_bts_km2']]\n",
    "\n",
    "    # Simulation: Home Location\n",
    "    df_loop = links[['grid_id', 'bts', 'distance', 'urbanity', 'f', 'h_tx', 'p_tx']].copy()\n",
    "\n",
    "    df_loop['alpha'] = np.vectorize(alpha_value)(df_loop['f'], df_loop['distance'], df_loop['h_tx'])\n",
    "    df_loop['path_loss_hata'] = np.vectorize(calculate_path_loss_hata_novar)(df_loop['f'], df_loop['urbanity'], df_loop['distance'], df_loop['h_tx'], 1, df_loop['alpha'])\n",
    "    \n",
    "    df_loop['rss'] = df_loop['p_tx'] - df_loop['path_loss_hata']\n",
    "\n",
    "    df_loop = df_loop.drop(columns = ['urbanity', 'f', 'h_tx', 'p_tx', 'path_loss_hata'])\n",
    "\n",
    "    df_loop = df_loop[df_loop.rss > -110]\n",
    "\n",
    "    df_loop.set_index('grid_id', inplace = True)\n",
    "    df_loop['w_best_ant'] = 0\n",
    "    df_loop.loc[df_loop.groupby('grid_id')['rss'].transform(max) == df_loop.rss, 'w_best_ant'] = 1\n",
    "    df_loop.reset_index(inplace=True)\n",
    "\n",
    "    temp = df_loop.groupby('grid_id')['w_best_ant'].sum().reset_index()\n",
    "\n",
    "    for i in temp[temp.w_best_ant > 1].grid_id:\n",
    "\n",
    "        if df_loop[df_loop.grid_id == i].w_best_ant.sum() > 1:\n",
    "\n",
    "            df_loop.loc[df_loop.grid_id == i, 'w_best_ant'] = np.where(df_loop[df_loop.grid_id == i].w_best_ant == 1, 1/df_loop[df_loop.grid_id == i].w_best_ant.sum(), 0)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    df_loop = df_loop.loc[df_loop['w_best_ant'] != 0, ['grid_id', 'bts', 'distance']]\n",
    "\n",
    "    # True home location\n",
    "    true_home = df_loop[['grid_id', 'bts']].copy()\n",
    "\n",
    "    df_loop = None\n",
    "\n",
    "    # P2P\n",
    "    bts_specs = gpd.GeoDataFrame(bts_specs, crs={'init': 'epsg:4326'}, geometry=[Point(xy) for xy in zip(bts_specs.x, bts_specs.y)])\n",
    "\n",
    "    map_p2p = gpd.tools.sjoin(bts_specs[['bts', 'geometry']], base_map, how=\"left\", op='intersects').drop(columns = ['index_right']).drop_duplicates('bts')\n",
    "\n",
    "    # Voronoi\n",
    "    bts_coords = np.array(\n",
    "        list(\n",
    "            [list(xy) for xy in zip(bts_specs.x, bts_specs.y)]))\n",
    "\n",
    "    poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(bts_coords, base_map.unary_union)\n",
    "\n",
    "    voronoi = gpd.GeoDataFrame(pd.DataFrame(poly_to_pt_assignments)[0].rename('voronoi_id'), crs = base_map.crs, geometry = poly_shapes)\n",
    "\n",
    "    voronoi['voronoi_km2'] = voronoi.area\n",
    "\n",
    "    voronoi = gpd.tools.sjoin(voronoi, bts_specs[['bts', 'geometry']], how=\"left\", op='intersects').drop(columns = ['index_right'])\n",
    "\n",
    "    map_voronoi = gpd.overlay(base_map, voronoi, how = 'intersection')\n",
    "    map_voronoi.insert(loc=0, column='intersection_id', value=list(range(len(map_voronoi))))\n",
    "\n",
    "    map_voronoi['intersection_km2'] = map_voronoi.area\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df[['grid_id','pop']], crs={'init': 'epsg:4326'}, geometry=[Point(xy) for xy in zip(df.x, df.y)])\n",
    "\n",
    "    voronoi_count = gpd.tools.sjoin(\n",
    "        map_voronoi, gdf[['pop', 'geometry']], how=\"left\", op='intersects').drop(\n",
    "        columns = ['index_right']).groupby('intersection_id')['pop'].count().reset_index()\n",
    "\n",
    "    map_voronoi = map_voronoi.merge(voronoi_count, on = 'intersection_id', how = 'left').rename(columns = {'pop' : 'intersection_count'})\n",
    "\n",
    "    voronoi_count = None\n",
    "\n",
    "    map_voronoi.set_index('voronoi_id', inplace = True)\n",
    "    map_voronoi['voronoi_count'] = map_voronoi.groupby('voronoi_id')['intersection_count'].sum()\n",
    "    map_voronoi.reset_index(inplace=True)\n",
    "\n",
    "    map_voronoi.set_index('area_id', inplace = True)\n",
    "    map_voronoi['area_count'] = map_voronoi.groupby('area_id')['intersection_count'].sum()\n",
    "    map_voronoi.reset_index(inplace=True)\n",
    "\n",
    "    vor_s_overlap = gpd.tools.sjoin(gdf, voronoi, how=\"left\", op='intersects').drop_duplicates(subset = 'grid_id')\n",
    "\n",
    "    gdf = None\n",
    "\n",
    "    map_voronoi['w_geo_vor_ant'] = map_voronoi['intersection_km2'] / map_voronoi['voronoi_km2']\n",
    "\n",
    "    map_voronoi['w_geo_vor_area'] = map_voronoi['intersection_km2'] / map_voronoi['area_km2']\n",
    "\n",
    "    map_voronoi['w_knn_vor_ant'] = map_voronoi['intersection_count'] / map_voronoi['voronoi_count']\n",
    "\n",
    "    map_voronoi['w_knn_vor_area'] = map_voronoi['intersection_count'] / map_voronoi['area_count']\n",
    "\n",
    "    #Simple HATA\n",
    "    hata_simple = links[['grid_id', 'bts', 'area_id', 'distance', 'urbanity', 'pop']].copy()\n",
    "\n",
    "    hata_simple.loc[hata_simple.urbanity == 0, 'f'] = 2100\n",
    "    hata_simple.loc[hata_simple.urbanity == 2, 'f'] = 900\n",
    "    hata_simple.loc[hata_simple.urbanity == 1, 'f'] = 900\n",
    "\n",
    "    hata_simple['h_tx'] = 30\n",
    "\n",
    "    hata_simple['p_tx'] = 45\n",
    "\n",
    "    hata_simple = hata_simple.astype({'urbanity' : 'uint8', 'f': 'uint16', 'h_tx' : 'uint8', 'p_tx' : 'uint8'})\n",
    "\n",
    "    hata_simple['alpha'] = np.vectorize(alpha_value)(hata_simple['f'], hata_simple['distance'], hata_simple['h_tx'])\n",
    "    hata_simple['path_loss'] = np.vectorize(calculate_path_loss_hata_novar)(hata_simple['f'], hata_simple['urbanity'], hata_simple['distance'], hata_simple['h_tx'], 1, hata_simple['alpha'])\n",
    "    \n",
    "    hata_simple['rss'] = hata_simple['p_tx'] - hata_simple['path_loss']\n",
    "\n",
    "    hata_simple_overlap = hata_simple[['bts', 'urbanity', 'f', 'h_tx', 'p_tx']].drop_duplicates()\n",
    "\n",
    "    hata_simple = hata_simple.drop(columns = ['alpha', 'path_loss', 'urbanity', 'f', 'h_tx', 'p_tx'])\n",
    "\n",
    "    hata_simple = hata_simple[hata_simple.rss > -110]\n",
    "\n",
    "    hata_simple.set_index('grid_id', inplace = True)\n",
    "    hata_simple['w_best_ant'] = 0\n",
    "    hata_simple.loc[hata_simple.groupby('grid_id')['rss'].transform(max) == hata_simple.rss, 'w_best_ant'] = 1\n",
    "    hata_simple.reset_index(inplace=True)\n",
    "\n",
    "    temp = hata_simple.groupby('grid_id')['w_best_ant'].sum().reset_index()\n",
    "\n",
    "    for i in temp[temp.w_best_ant > 1].grid_id:\n",
    "        if hata_simple[hata_simple.grid_id == i].w_best_ant.sum() > 1:\n",
    "            hata_simple.loc[hata_simple.grid_id == i, 'w_best_ant'] = np.where(hata_simple[hata_simple.grid_id == i].w_best_ant == 1, 1/hata_simple[hata_simple.grid_id == i].w_best_ant.sum(), 0)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    hata_simple_best = hata_simple.loc[hata_simple['w_best_ant'] != 0, ['grid_id', 'bts', 'distance', 'w_best_ant', 'pop']]\n",
    "\n",
    "    hata_simple['rss_inv'] = 1 / (hata_simple.rss)**2\n",
    "\n",
    "    hata_simple.set_index('grid_id', inplace = True)\n",
    "    hata_simple['rss_sum_inv'] = hata_simple.groupby('grid_id')['rss_inv'].sum()\n",
    "    hata_simple['w_knn_ant'] = hata_simple.rss_inv / hata_simple.rss_sum_inv\n",
    "    hata_simple.reset_index(inplace=True)\n",
    "\n",
    "    hata_simple.set_index('bts', inplace = True)\n",
    "    hata_simple['w_knn_ant_pop'] = hata_simple.w_knn_ant # * hata_simple['pop_avg']\n",
    "    hata_simple['w_knn_ant_sum'] = hata_simple.groupby('bts')['w_knn_ant_pop'].sum()\n",
    "    hata_simple['w_knn_site'] = hata_simple.w_knn_ant_pop / hata_simple.w_knn_ant_sum\n",
    "    hata_simple.reset_index(inplace=True)\n",
    "\n",
    "    hata_simple.set_index('area_id', inplace = True)\n",
    "    hata_simple['w_knn_ant_sum'] = hata_simple.groupby('area_id')['w_knn_ant_pop'].sum()\n",
    "    hata_simple['w_knn_area'] = hata_simple.w_knn_ant_pop / hata_simple.w_knn_ant_sum\n",
    "    hata_simple.reset_index(inplace=True)\n",
    "\n",
    "    # Overlaps with True Coverage\n",
    "    ### P2P\n",
    "    p2p_a_overlap = true_coverage.merge(map_p2p[['bts', 'area_id', 'area_km2']].rename(columns = {'area_id' : 'true_bts_area_id'}), left_on = 'true_bts', right_on = 'bts', how ='left')\n",
    "\n",
    "    p2p_a_overlap = gpd.overlay(p2p_a_overlap, base_map[['geometry', 'area_id']], how = 'intersection')\n",
    "\n",
    "    p2p_a_overlap['intersection_km2'] = p2p_a_overlap.area\n",
    "\n",
    "    p2p_a_overlap = (p2p_a_overlap[p2p_a_overlap.area_id == p2p_a_overlap.true_bts_area_id].groupby('true_bts')['intersection_km2'].sum() /\n",
    "                     (p2p_a_overlap.groupby('true_bts')['true_bts_km2'].mean() + \n",
    "                      p2p_a_overlap[p2p_a_overlap.area_id == p2p_a_overlap.true_bts_area_id].groupby('true_bts')['area_km2'].mean() - \n",
    "                      p2p_a_overlap[p2p_a_overlap.area_id == p2p_a_overlap.true_bts_area_id].groupby('true_bts')['intersection_km2'].sum()\n",
    "                     )\n",
    "                    ).rename('p2p_a_overlap').reset_index()\n",
    "\n",
    "    df_eval_cov = df_eval_cov.merge(p2p_a_overlap, on = 'true_bts', how = 'left')\n",
    "\n",
    "    s_per_area = df[['area_id', 'pop']].groupby('area_id').count().reset_index().rename(columns = {'area_id' : 'area_id_id', 'pop' : 'pop_per_area'})\n",
    "\n",
    "    p2p_s_overlap = df[['grid_id', 'area_id', 'pop']].merge(true_home.rename(columns = {'bts' : 'true_bts'}), on = 'grid_id', how = 'left')\n",
    "\n",
    "    p2p_s_overlap = p2p_s_overlap.merge(map_p2p[['bts', 'area_id']].rename(columns = {'area_id' : 'true_bts_area_id'}), left_on = 'true_bts', right_on = 'bts', how ='left')\n",
    "\n",
    "    p2p_s_overlap = p2p_s_overlap.merge(s_per_area, left_on = 'true_bts_area_id', right_on = 'area_id_id', how = 'left')\n",
    "\n",
    "    p2p_s_overlap = (p2p_s_overlap[p2p_s_overlap.area_id == p2p_s_overlap.true_bts_area_id].groupby('true_bts')['pop'].count() / \n",
    "                     (p2p_s_overlap.groupby('true_bts')['pop'].count() + \n",
    "                      p2p_s_overlap[p2p_s_overlap.area_id == p2p_s_overlap.true_bts_area_id].groupby('true_bts')['pop_per_area'].mean() - \n",
    "                      p2p_s_overlap[p2p_s_overlap.area_id == p2p_s_overlap.true_bts_area_id].groupby('true_bts')['pop'].count()\n",
    "                     )\n",
    "                    ).rename('p2p_s_overlap').reset_index().fillna(0)\n",
    "\n",
    "    df_eval_cov = df_eval_cov.merge(p2p_s_overlap, on = 'true_bts', how = 'left')\n",
    "\n",
    "    ### Voronoi\n",
    "    vor_overlap = gpd.overlay(true_coverage, voronoi, how = 'intersection')\n",
    "\n",
    "    vor_overlap['intersection_km2'] = vor_overlap.area\n",
    "\n",
    "    vor_a_overlap = (vor_overlap[vor_overlap.true_bts == vor_overlap.bts].groupby('true_bts')['intersection_km2'].sum() / \n",
    "                     (vor_overlap.groupby('true_bts')['true_bts_km2'].mean() +\n",
    "                      vor_overlap[vor_overlap.true_bts == vor_overlap.bts].groupby('true_bts')['voronoi_km2'].mean() -\n",
    "                      vor_overlap[vor_overlap.true_bts == vor_overlap.bts].groupby('true_bts')['intersection_km2'].sum()\n",
    "                     )\n",
    "                    ).rename('vor_a_overlap').reset_index()\n",
    "\n",
    "    df_eval_cov = df_eval_cov.merge(vor_a_overlap, on = 'true_bts', how = 'left')\n",
    "\n",
    "    vor_s_overlap = vor_s_overlap[['grid_id', 'bts', 'pop']].merge(true_home.rename(columns = {'bts' : 'true_bts'}), on = 'grid_id', how = 'left')\n",
    "\n",
    "    vor_s_overlap = (vor_s_overlap[vor_s_overlap.bts == vor_s_overlap.true_bts].groupby('true_bts')['pop'].count() / \n",
    "                     (vor_s_overlap.groupby('true_bts')['pop'].count() +\n",
    "                      vor_s_overlap.groupby('bts')['pop'].count() -\n",
    "                      vor_s_overlap[vor_s_overlap.bts == vor_s_overlap.true_bts].groupby('true_bts')['pop'].count()\n",
    "                     )\n",
    "                    ).reset_index().fillna(0).rename(columns = {'index' : 'true_bts', 'pop' : 'vor_s_overlap'})\n",
    "\n",
    "    df_eval_cov = df_eval_cov.merge(vor_s_overlap, on = 'true_bts', how = 'left')\n",
    "\n",
    "    ### Simple HATA\n",
    "    for i in hata_simple_overlap.index:\n",
    "\n",
    "        for j in np.arange(0, 100, 0.1):\n",
    "\n",
    "            alpha = alpha_value(hata_simple_overlap.loc[i, 'f'], j, hata_simple_overlap.loc[i, 'h_tx'])\n",
    "            path_loss = calculate_path_loss_hata_novar(hata_simple_overlap.loc[i, 'f'], hata_simple_overlap.loc[i, 'urbanity'], j, hata_simple_overlap.loc[i, 'h_tx'], 1, alpha)\n",
    "\n",
    "            if (hata_simple_overlap.loc[i, 'p_tx'] - path_loss) < -110:\n",
    "                hata_simple_overlap.loc[i, 'max_range'] = j\n",
    "                break\n",
    "\n",
    "    hata_simple_overlap = hata_simple_overlap[['bts', 'max_range']].rename(columns = {'max_range' : 'distance'})\n",
    "\n",
    "    hata_simple_overlap = hata_simple_overlap[hata_simple_overlap['distance'] != 0]\n",
    "\n",
    "    hata_simple_overlap = hata_simple_overlap.merge(bts_specs[['bts', 'x', 'y']], on = 'bts', how = 'left')\n",
    "\n",
    "    hata_simple_overlap = gpd.GeoDataFrame(hata_simple_overlap,\n",
    "                                      crs = {'init': 'epsg:4326'},\n",
    "                                      geometry=[Point(xy) for xy in zip(hata_simple_overlap.x, hata_simple_overlap.y)])\n",
    "\n",
    "    hata_simple_overlap['geometry'] = hata_simple_overlap.apply(lambda x : x.geometry.buffer(x.distance), axis = 1)\n",
    "\n",
    "    hata_simple_overlap['bts_km2'] = hata_simple_overlap.area\n",
    "\n",
    "    hata_simple_a_overlap = gpd.overlay(true_coverage, hata_simple_overlap, how = 'intersection')\n",
    "\n",
    "    hata_simple_a_overlap['intersection_km2'] = hata_simple_a_overlap.area\n",
    "\n",
    "    hata_simple_a_overlap = (hata_simple_a_overlap[hata_simple_a_overlap.true_bts == hata_simple_a_overlap.bts].groupby('true_bts')['intersection_km2'].sum() / \n",
    "                     (hata_simple_a_overlap.groupby('true_bts')['true_bts_km2'].mean() +\n",
    "                      hata_simple_a_overlap[hata_simple_a_overlap.true_bts == hata_simple_a_overlap.bts].groupby('true_bts')['bts_km2'].mean() -\n",
    "                      hata_simple_a_overlap[hata_simple_a_overlap.true_bts == hata_simple_a_overlap.bts].groupby('true_bts')['intersection_km2'].sum()\n",
    "                     )\n",
    "                    ).rename('hata_simple_a_overlap').reset_index()\n",
    "\n",
    "    df_eval_cov = df_eval_cov.merge(hata_simple_a_overlap, on = 'true_bts', how = 'left')\n",
    "\n",
    "    hata_simple_s_overlap = hata_simple_best[['bts', 'grid_id', 'pop']].merge(true_home.rename(columns = {'bts' : 'true_bts'}), on = 'grid_id', how = 'left')\n",
    "\n",
    "    hata_simple_s_overlap = (hata_simple_s_overlap[hata_simple_s_overlap.bts == hata_simple_s_overlap.true_bts].groupby('true_bts')['pop'].count() / \n",
    "                     (hata_simple_s_overlap.groupby('true_bts')['pop'].count() +\n",
    "                      hata_simple_s_overlap.groupby('bts')['pop'].count() -\n",
    "                      hata_simple_s_overlap[hata_simple_s_overlap.bts == hata_simple_s_overlap.true_bts].groupby('true_bts')['pop'].count()\n",
    "                     )\n",
    "                    ).reset_index().rename(columns = {'index' : 'true_bts', 'id' : 'hata_simple_s_overlap'})\n",
    "\n",
    "    df_eval_cov = df_eval_cov.merge(hata_simple_s_overlap, on = 'true_bts', how = 'left')\n",
    "    \n",
    "    df_eval_cov['run'] = r\n",
    "\n",
    "    # Estimation Effects\n",
    "    est_effects = true_home.rename(columns = {'bts' : 'true_bts'}).merge(df[['grid_id', 'area_id', 'pop', 'poverty_model']], on = 'grid_id', how = 'left')\n",
    "\n",
    "    est_effects_bts = est_effects.groupby('true_bts')['pop', 'poverty_model'].agg(\n",
    "        lambda x: np.average(x, weights=est_effects.loc[x.index, \"pop\"])).reset_index().drop(columns = ['pop'])\n",
    "\n",
    "    df_eval_est = pd.DataFrame(base_map['area_id'].unique(), columns = ['area_id'])\n",
    "\n",
    "    ### True Poverty\n",
    "    df_eval_est_true = df.groupby('area_id')['pop', 'poverty_rate'].agg(\n",
    "        lambda x: np.average(x, weights=df.loc[x.index, \"pop\"])).reset_index().drop(columns = ['pop'])\n",
    "\n",
    "    df_eval_est = df_eval_est.merge(df_eval_est_true, on = 'area_id', how = 'left')\n",
    "\n",
    "    # True Home (Benchmark)\n",
    "    est_effects_true = est_effects.drop(columns = ['poverty_model']).merge(est_effects_bts, on = 'true_bts', how = 'left')\n",
    "\n",
    "    est_effects_true = est_effects_true.dropna().groupby('area_id')['poverty_model'].mean().reset_index().rename(columns = {'poverty_model' : 'p_true'})\n",
    "\n",
    "    df_eval_est = df_eval_est.merge(est_effects_true, on = 'area_id', how = 'left')\n",
    "\n",
    "    ### P2P\n",
    "    est_effects_p2p = est_effects_bts.merge(map_p2p[['bts', 'area_id']], left_on = 'true_bts', right_on = 'bts', how = 'left')\n",
    "\n",
    "    est_effects_p2p = est_effects_p2p.dropna().groupby('area_id')['poverty_model'].mean().rename('p_p2p').reset_index()\n",
    "\n",
    "    df_eval_est = df_eval_est.merge(est_effects_p2p, on = 'area_id', how = 'left')\n",
    "\n",
    "    ### Voronoi\n",
    "    est_effects_vor = map_voronoi[['bts', 'area_id', 'w_geo_vor_area']].merge(\n",
    "        est_effects_bts, left_on = 'bts', right_on = 'true_bts', how = 'left')\n",
    "\n",
    "    est_effects_vor = est_effects_vor.dropna().groupby('area_id')['w_geo_vor_area', 'poverty_model'].agg(\n",
    "        lambda x: np.average(x, weights=est_effects_vor.loc[x.index, \"w_geo_vor_area\"])).reset_index().drop(columns = ['w_geo_vor_area']).rename(columns = {'poverty_model' : 'p_vor'})\n",
    "\n",
    "    df_eval_est = df_eval_est.merge(est_effects_vor, on = 'area_id', how = 'left')\n",
    "\n",
    "    ### Augmented Voronoi\n",
    "    est_effects_a_vor = map_voronoi[['bts', 'area_id', 'w_knn_vor_area']].merge(\n",
    "        est_effects_bts, left_on = 'bts', right_on = 'true_bts', how = 'left')\n",
    "\n",
    "    est_effects_a_vor = est_effects_a_vor[est_effects_a_vor.w_knn_vor_area != 0]\n",
    "\n",
    "    est_effects_a_vor = est_effects_a_vor.dropna().groupby('area_id')['w_knn_vor_area', 'poverty_model'].agg(\n",
    "        lambda x: np.average(x, weights=est_effects_a_vor.loc[x.index, \"w_knn_vor_area\"])).reset_index().drop(columns = ['w_knn_vor_area']).rename(columns = {'poverty_model' : 'p_a_vor'})\n",
    "\n",
    "    df_eval_est = df_eval_est.merge(est_effects_a_vor, on = 'area_id', how = 'left')\n",
    "\n",
    "    ### Simple HATA BSA\n",
    "    est_effects_bsa_simple = hata_simple[['grid_id', 'bts', 'area_id', 'w_best_ant']].merge(est_effects_bts, left_on = 'bts', right_on = 'true_bts', how = 'left')\n",
    "\n",
    "    est_effects_bsa_simple = est_effects_bsa_simple[est_effects_bsa_simple.w_best_ant != 0]\n",
    "\n",
    "    est_effects_bsa_simple = est_effects_bsa_simple.dropna().groupby('area_id')['w_best_ant', 'poverty_model'].agg(\n",
    "        lambda x: np.average(x, weights=est_effects_bsa_simple.loc[x.index, \"w_best_ant\"])).reset_index().drop(columns = ['w_best_ant']).rename(columns = {'poverty_model' : 'p_bsa_simple'})\n",
    "\n",
    "    df_eval_est = df_eval_est.merge(est_effects_bsa_simple, on = 'area_id', how = 'left')\n",
    "\n",
    "    ### Simple HATA IDW\n",
    "    est_effects_idw_simple = hata_simple[['grid_id', 'bts', 'area_id', 'w_knn_area']].merge(est_effects_bts, left_on = 'bts', right_on = 'true_bts', how = 'left')\n",
    "\n",
    "    est_effects_idw_simple = est_effects_idw_simple[est_effects_idw_simple.w_knn_area != 0]\n",
    "\n",
    "    est_effects_idw_simple = est_effects_idw_simple.dropna().groupby('area_id')['w_knn_area', 'poverty_model'].agg(\n",
    "        lambda x: np.average(x, weights=est_effects_idw_simple.loc[x.index, \"w_knn_area\"])).reset_index().drop(columns = ['w_knn_area']).rename(columns = {'poverty_model' : 'p_idw_simple'})\n",
    "\n",
    "    df_eval_est = df_eval_est.merge(est_effects_idw_simple, on = 'area_id', how = 'left')\n",
    "    \n",
    "    df_eval_est['run'] = r\n",
    "    \n",
    "    return(df_eval_cov.values.tolist(), df_eval_est.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_results(result):\n",
    "    \"\"\"Uses apply_async's callback to setup up a separate Queue for each process\"\"\"\n",
    "    output_eval_cov.extend(result[0])\n",
    "    output_eval_est.extend(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_eval_cov = []\n",
    "output_eval_est = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()  \n",
    "    \n",
    "    # Repeats the compute intensive operation on 7 data frames concurrently\n",
    "    pool = mp.Pool(processes=mp.cpu_count()-1)\n",
    "    [pool.apply_async(simulation_small, args=(df_full, pixel_x, pixel_y, park_x_low, park_y_low, park_x_high, park_y_high,\n",
    "                     pop_urban, pop_rural, r), callback=collect_results) for r in range(1000)]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Converts list of lists to a data frame\n",
    "    eval_cov = pd.DataFrame(output_eval_cov, columns = [\n",
    "        'true_bts', 'urbanity', 'true_bts_km2', 'p2p_a_overlap', 'p2p_s_overlap', 'vor_a_overlap', 'vor_s_overlap', 'hata_simple_a_overlap', 'hata_simple_s_overlap', 'run'])\n",
    "    eval_est = pd.DataFrame(output_eval_est, columns = [\n",
    "        'area_id', 'poverty_rate', 'p_true', 'p_p2p', 'p_vor', 'p_a_vor', 'p_bsa_simple', 'p_idw_simple', 'run'])\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_final = pd.DataFrame({'Mean_T' : np.round(eval_cov.mean(numeric_only = True), decimals = 3),\n",
    "                              #'sigma_T' : np.round(eval_cov.std(numeric_only = True), decimals = 3),\n",
    "                              'Mean_R' : np.round(eval_cov[eval_cov.urbanity == 1].mean(numeric_only = True), decimals = 3),\n",
    "                              #'sigma_R' : np.round(eval_cov[eval_cov.urbanity == 1].std(numeric_only = True), decimals = 3),\n",
    "                              'Mean_S' : np.round(eval_cov[eval_cov.urbanity == 2].mean(numeric_only = True), decimals = 3),\n",
    "                              #'sigma_S' : np.round(eval_cov[eval_cov.urbanity == 2].std(numeric_only = True), decimals = 3),\n",
    "                              'Mean_U' : np.round(eval_cov[eval_cov.urbanity == 0].mean(numeric_only = True), decimals = 3),\n",
    "                              #'sigma_U' : np.round(eval_cov[eval_cov.urbanity == 0].std(numeric_only = True), decimals = 3)\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = eval_est.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rank = pd.DataFrame()\n",
    "\n",
    "for m in tqdm(df_predict.columns.to_series().drop(['area_id', 'poverty_rate', 'run', 'p_true']).to_list()):\n",
    " \n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    for i in df_predict['run'].unique():\n",
    "        \n",
    "        temp = df_predict[df_predict['run'] == i].dropna().copy()\n",
    "        \n",
    "        result.loc[i, 'run'] = i\n",
    "        \n",
    "        result.loc[i, 'bias'] = (temp[m] - temp['poverty_rate']).mean()\n",
    "        result.loc[i, 'rmse'] = np.sqrt(((temp[m] - temp['poverty_rate'])**2).mean())\n",
    "        result.loc[i, 'model'] = m\n",
    "        \n",
    "        temp2 = temp[['poverty_rate', m]]\n",
    "        x = np.array(temp2[m]).reshape((-1, 1))\n",
    "        y = np.array(temp2.poverty_rate)\n",
    "        result.loc[i, 'r2'] = LinearRegression().fit(x, y).score(x, y)\n",
    "        \n",
    "    prediction_rank = prediction_rank.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rank['bias'] = abs(prediction_rank['bias'])\n",
    "prediction_rank['rmse'] = np.sqrt(prediction_rank['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rank['best_r2'] = 0\n",
    "prediction_rank.loc[prediction_rank.groupby('run')['r2'].transform(max) == prediction_rank.r2, 'best_r2'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rank.groupby('model')['best_r2'].mean().reset_index().sort_values(by=['best_r2']).to_latex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rank['best_bias'] = 0\n",
    "prediction_rank.loc[prediction_rank.groupby('run')['bias'].transform(min) == prediction_rank.bias, 'best_bias'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rank.groupby('model')['best_bias'].mean().reset_index().sort_values(by=['best_bias']).to_latex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rank['best_rmse'] = 0\n",
    "prediction_rank.loc[prediction_rank.groupby('run')['rmse'].transform(min) == prediction_rank.rmse, 'best_rmse'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rank.groupby('model')['best_rmse'].mean().reset_index().sort_values(by=['best_rmse']).to_latex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking - Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coverage = eval_cov.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_rank = pd.DataFrame()\n",
    "\n",
    "for m in tqdm(df_coverage.columns.to_series().drop(['true_bts', 'urbanity', 'true_bts_km2', 'run', 'p2p_s_overlap', 'vor_s_overlap', 'hata_simple_s_overlap']).to_list()):\n",
    " \n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    for i in df_coverage['run'].unique():\n",
    "        \n",
    "        temp = df_coverage[df_coverage['run'] == i].copy()\n",
    "        \n",
    "        result.loc[i, 'coverage'] = temp[m].mean()\n",
    "        result.loc[i, 'run'] = i\n",
    "        result.loc[i, 'model'] = m\n",
    "        result.loc[i, 'type'] = 'geography'\n",
    "        \n",
    "    coverage_rank = coverage_rank.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in tqdm(df_coverage.columns.to_series().drop(['true_bts', 'urbanity', 'true_bts_km2', 'run', 'p2p_a_overlap', 'vor_a_overlap', 'hata_simple_a_overlap']).to_list()):\n",
    " \n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    for i in df_coverage['run'].unique():\n",
    "        \n",
    "        temp = df_coverage[df_coverage['run'] == i].copy()\n",
    "        \n",
    "        result.loc[i, 'coverage'] = temp[m].mean()\n",
    "        result.loc[i, 'run'] = i\n",
    "        result.loc[i, 'model'] = m\n",
    "        result.loc[i, 'type'] = 'settlement'\n",
    "        \n",
    "    coverage_rank = coverage_rank.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_rank_geo = coverage_rank[coverage_rank['type'] == 'geography'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_rank_geo['best_geo'] = 0\n",
    "coverage_rank_geo.loc[coverage_rank_geo.groupby('run')['coverage'].transform(max) == coverage_rank_geo.coverage, 'best_geo'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_rank_geo.groupby('model')['best_geo'].mean().reset_index().sort_values(by=['best_geo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settlements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_rank_stl = coverage_rank[coverage_rank['type'] == 'settlement'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_rank_stl['best_geo'] = 0\n",
    "coverage_rank_stl.loc[coverage_rank_stl.groupby('run')['coverage'].transform(max) == coverage_rank_stl.coverage, 'best_geo'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_rank_stl.groupby('model')['best_geo'].mean().reset_index().sort_values(by=['best_geo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation & Sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_est_final = eval_est.merge(base_map[['area_id', 'area_type']], on = 'area_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_est_final = pd.DataFrame({'Corr' : np.round(eval_est_final.corr()['poverty_rate'], decimals = 3),\n",
    "              'N_Total' : np.round(eval_est_final.count()/1000).astype('int'),\n",
    "              'Corr_Rural' : np.round(eval_est_final[eval_est_final['area_type'] == 1].corr()['poverty_rate'], decimals = 3),\n",
    "              'N_Rural' : np.round(eval_est_final[eval_est_final['area_type'] == 1].count()/1000).astype('int'),\n",
    "              'Corr_Urban' : np.round(eval_est_final[eval_est_final['area_type'] == 0].corr()['poverty_rate'], decimals = 3),\n",
    "              'N_Urban' : np.round(eval_est_final[eval_est_final['area_type'] == 0].count()/1000).astype('int')\n",
    "             })[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = eval_est.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(df_predict['run'].unique()):\n",
    "    \n",
    "    temp = df_predict[df_predict['run'] == i].dropna().copy()\n",
    "    \n",
    "    result.loc[i, 'run'] = i\n",
    "    \n",
    "    for j in temp.columns.to_series().drop(['area_id', 'poverty_rate', 'run']).to_list():\n",
    "         \n",
    "        result.loc[i, str(j+'_bias')] = (temp[j] - temp['poverty_rate']).mean()\n",
    "        result.loc[i, str(j+'_rmse')] = np.sqrt(((temp[j] - temp['poverty_rate'])**2).mean())\n",
    "        \n",
    "        temp2 = temp[['poverty_rate', j]]\n",
    "        x = np.array(temp2[j]).reshape((-1, 1))\n",
    "        y = np.array(temp2.poverty_rate)\n",
    "        \n",
    "        result.loc[i, str(j+'_r2')] = LinearRegression().fit(x, y).score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(15, 5))\n",
    "sns.boxplot(data = result[[\n",
    "    'p_true_r2', 'p_p2p_r2', 'p_vor_r2', 'p_a_vor_r2', 'p_bsa_simple_r2', 'p_idw_simple_r2']].rename(columns = {\n",
    "    'p_true_r2' : 'Benchmark', 'p_p2p_r2' : 'Point-to-Polygon', 'p_vor_r2' : 'Voronoi', 'p_a_vor_r2' : 'Augmented \\n Voronoi', \n",
    "    'p_bsa_simple_r2' : 'Simple HATA \\n (BSA)', 'p_idw_simple_r2' : 'Simple HATA \\n (IDW)'}),\n",
    "            palette = 'rocket')\n",
    "\n",
    "ax.tick_params(labelsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(15, 5))\n",
    "sns.boxplot(data = result[[\n",
    "    'p_true_bias', 'p_p2p_bias', 'p_vor_bias', 'p_a_vor_bias', 'p_bsa_simple_bias', 'p_idw_simple_bias']].rename(columns = {\n",
    "    'p_true_bias' : 'Benchmark', 'p_p2p_bias' : 'Point-to-Polygon', 'p_vor_bias' : 'Voronoi', 'p_a_vor_bias' : 'Augmented \\n Voronoi', \n",
    "    'p_bsa_simple_bias' : 'Simple HATA \\n (BSA)', 'p_idw_simple_bias' : 'Simple HATA \\n (IDW)'}),\n",
    "            palette = 'rocket')\n",
    "\n",
    "ax.tick_params(labelsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(15, 5))\n",
    "sns.boxplot(data = result[[\n",
    "    'p_true_rmse', 'p_p2p_rmse', 'p_vor_rmse', 'p_a_vor_rmse', 'p_bsa_simple_rmse', 'p_idw_simple_rmse']].rename(columns = {\n",
    "    'p_true_rmse' : 'Benchmark', 'p_p2p_rmse' : 'Point-to-Polygon', 'p_vor_rmse' : 'Voronoi', 'p_a_vor_rmse' : 'Augmented \\n Voronoi', \n",
    "    'p_bsa_simple_rmse' : 'Simple HATA \\n (BSA)', 'p_idw_simple_rmse' : 'Simple HATA \\n (IDW)'}),\n",
    "            palette = 'rocket')\n",
    "\n",
    "ax.tick_params(labelsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = base_map.merge(eval_est[eval_est['run'] == 1], on = 'area_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['delta_p_true'] = results['poverty_rate'] - results['p_true']\n",
    "results['delta_p_p2p'] = results['poverty_rate'] - results['p_p2p']\n",
    "results['delta_p_vor'] = results['poverty_rate'] - results['p_vor']\n",
    "results['delta_p_bsa_simple'] = results['poverty_rate'] - results['p_bsa_simple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, figsize=(10, 5))\n",
    "\n",
    "results[['geometry', 'poverty_rate']].dropna().plot(ax = ax1, column = 'poverty_rate', cmap = plt.get_cmap(\"rocket\"), vmin = 0, vmax = 1, linewidth = 0.8, edgecolor = 'black')\n",
    "ax1.set_title('True Poverty Rate')\n",
    "results[['geometry', 'delta_p_true']].dropna().plot(ax = ax2, column = 'delta_p_true', cmap = plt.get_cmap(\"rocket\"), vmin = 0, vmax = 1, linewidth = 0.8, edgecolor = 'black')\n",
    "ax2.set_title('Benchmark')\n",
    "results[['geometry', 'delta_p_p2p']].dropna().plot(ax = ax3, column = 'delta_p_p2p', cmap = plt.get_cmap(\"rocket\"), vmin = 0, vmax = 1, linewidth = 0.8, edgecolor = 'black')\n",
    "ax3.set_title('Point-to-Polygon')\n",
    "results[['geometry', 'delta_p_vor']].dropna().plot(ax = ax4, column = 'delta_p_vor', cmap = plt.get_cmap(\"rocket\"), vmin = 0, vmax = 1, linewidth = 0.8, edgecolor = 'black')\n",
    "ax4.set_title('Voronoi')\n",
    "results[['geometry', 'delta_p_bsa_simple']].dropna().plot(ax = ax5, column = 'delta_p_bsa_simple', cmap = plt.get_cmap(\"rocket\"), vmin = 0, vmax = 1, linewidth = 0.8, edgecolor = 'black')\n",
    "ax5.set_title('Simple HATA')\n",
    "\n",
    "for ax in f.get_axes():\n",
    "    ax.label_outer()\n",
    "\n",
    "    \n",
    "f.subplots_adjust(right=0.8)\n",
    "im = plt.cm.ScalarMappable(cmap = plt.get_cmap(\"rocket\"), norm = plt.Normalize(vmin = 0, vmax = 1))\n",
    "im._A = []\n",
    "cbar_ax = f.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "f.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
